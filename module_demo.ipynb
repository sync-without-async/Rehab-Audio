{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import denoising as den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DSR_MODULE]2023-10-03 07:58:34,719 INFO Loading model...\n",
      "[DSR_MODULE]2023-10-03 07:58:36,083 INFO Converting audio...\n",
      "[DSR_MODULE]2023-10-03 07:58:36,128 INFO Inference...\n",
      "[DSR_MODULE]2023-10-03 07:58:36,546 INFO Converting output...\n",
      "[DSR_MODULE]2023-10-03 07:58:36,550 INFO Done!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_AUDIO = \"./doc_and_pat_data/dhl_doctor_with_isb.wav\"\n",
    "data, fs = den.load_audio(DATA_AUDIO)\n",
    "\n",
    "# Denoising dhl doctor audio\n",
    "doctor_denoised_audio, doctor_sr = den.denoising(\n",
    "    audio=data,\n",
    "    sample_rate=fs,\n",
    "    device=\"cuda\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DSR_MODULE]2023-10-03 07:58:36,638 INFO Loading model...\n",
      "[DSR_MODULE]2023-10-03 07:58:37,037 INFO Converting audio...\n",
      "[DSR_MODULE]2023-10-03 07:58:37,148 INFO Inference...\n",
      "[DSR_MODULE]2023-10-03 07:58:37,361 INFO Converting output...\n",
      "[DSR_MODULE]2023-10-03 07:58:37,363 INFO Done!\n"
     ]
    }
   ],
   "source": [
    "DATA_AUDIO = \"./doc_and_pat_data/isb_patient_with_dlh.wav\"\n",
    "data, fs = den.load_audio(DATA_AUDIO)\n",
    "\n",
    "# Denoising isb patient audio\n",
    "patient_denoised_audio, patient_sr = den.denoising(\n",
    "    audio=data,\n",
    "    sample_rate=fs,\n",
    "    device=\"cuda\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech To Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DSR_MODULE]2023-10-03 07:58:38,639 INFO Created a temporary directory at /tmp/tmprwgf6r7b\n",
      "[DSR_MODULE]2023-10-03 07:58:38,642 INFO Writing /tmp/tmprwgf6r7b/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "import speech_to_text as stt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STT with fine-tunined Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DSR_MODULE]2023-10-03 07:58:40,184 INFO device argument is not provided. Using default value: cuda\n",
      "[DSR_MODULE]2023-10-03 07:58:40,186 INFO Loading model...\n",
      "[DSR_MODULE]2023-10-03 07:58:57,884 INFO Converting audio...\n",
      "[DSR_MODULE]2023-10-03 07:58:57,959 INFO Inference...\n",
      "[DSR_MODULE]2023-10-03 07:59:00,362 INFO Decoding...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요 환자분 오늘 어떤 문제로 대언하셨나요 발열과 두통은 언제부터 시작되었나요 발열과 두통이 있을 때는 어떤 증상이 나타나나요 그 외에 다른 증상은 없나요']\n"
     ]
    }
   ],
   "source": [
    "doctor_text_whisper = stt.speech_to_text_whisper(\n",
    "    pretrained_model_name_or_path=\"byoussef/whisper-large-v2-Ko\",\n",
    "    audio=doctor_denoised_audio,\n",
    "    audio_sample_rate=doctor_sr,\n",
    "    verbose=True\n",
    ")\n",
    "print(doctor_text_whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요 환자분 오늘 어떤 문제로 대언하셨나요 발열과 두통은 언제부터 시작되었나요 발열과 두통이 있을 때는 어떤 증상이 나타나나요 그 외에 다른 증상은 없나요']\n"
     ]
    }
   ],
   "source": [
    "print(doctor_text_whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DSR_MODULE]2023-10-03 07:59:00,417 INFO device argument is not provided. Using default value: cuda\n",
      "[DSR_MODULE]2023-10-03 07:59:00,419 INFO Loading model...\n",
      "[DSR_MODULE]2023-10-03 07:59:16,717 INFO Converting audio...\n",
      "[DSR_MODULE]2023-10-03 07:59:16,787 INFO Inference...\n",
      "[DSR_MODULE]2023-10-03 07:59:19,369 INFO Decoding...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요 의사 선생님 저는 요즘 발열과 두통이 심해서 맞습니다 며칠 전부터 시작되었는데 점점 심해지는 것 같아요 열이 나고 머리가 아프고 목이 아픕니다 콧물이 나고 기침이 납니다']\n"
     ]
    }
   ],
   "source": [
    "patient_text_whisper = stt.speech_to_text_whisper(\n",
    "    pretrained_model_name_or_path=\"byoussef/whisper-large-v2-Ko\",\n",
    "    audio=patient_denoised_audio,\n",
    "    audio_sample_rate=patient_sr,\n",
    "    verbose=True\n",
    ")\n",
    "print(patient_text_whisper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline STT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DSR_MODULE]2023-10-03 07:59:19,437 INFO Loading model...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[DSR_MODULE]2023-10-03 07:59:22,916 INFO Converting audio...\n",
      "[DSR_MODULE]2023-10-03 07:59:22,928 INFO Inference...\n",
      "[DSR_MODULE]2023-10-03 07:59:22,989 INFO Decoding...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['만냐 제 환자뿐는 오늘 어떤 문제로 태원하셨나버를 같 뜻 뿐을 얹제붙트 시작 때나어 을 가 트통이 있을 때는 어떤 증상이 나타나아그 에 른 생상을 없나내 알겠습니다 헌자분의 증상을 듣고 패려이 의심 됩니 하지만 비대이 진류이기 때문에 정확한 진단을 내리가 어렵습니다 혹시 가까운 병원에 방문하실 수 있나내 그렇다면 가까운 병원에 방문하셨서 진료를 받으시기 바랍니다 배려이 의심된다면 신부 액세레이를 치어 보면 정확한 진단을 내릴 수 있을 것입니내요']\n"
     ]
    }
   ],
   "source": [
    "doctor_text = stt.speech_to_text(\n",
    "    processor_pretrained_argument=\"kresnik/wav2vec2-large-xlsr-korean\",\n",
    "    audio=doctor_denoised_audio,\n",
    "    audio_sample_rate=doctor_sr,\n",
    "    device=\"cuda\",\n",
    "    verbose=True)\n",
    "print(doctor_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DSR_MODULE]2023-10-03 07:59:23,247 INFO Loading model...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[DSR_MODULE]2023-10-03 07:59:26,829 INFO Converting audio...\n",
      "[DSR_MODULE]2023-10-03 07:59:26,840 INFO Inference...\n",
      "[DSR_MODULE]2023-10-03 07:59:26,896 INFO Decoding...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['냐 세 의사상인 저는 요지 발월과 통의 심에서 왔습니매치제부터 시작되였는데 점점 시해 지는 것 같아요여린리 나고 머리가 부고 보기 아습니콧물이나고 기친이 합니내 방문하겠습니날겠습니다 한사니다다']\n"
     ]
    }
   ],
   "source": [
    "patient_text = stt.speech_to_text(\n",
    "    processor_pretrained_argument=\"kresnik/wav2vec2-large-xlsr-korean\",\n",
    "    audio=patient_denoised_audio,\n",
    "    audio_sample_rate=patient_sr,\n",
    "    device=\"cuda\",\n",
    "    verbose=True)\n",
    "print(patient_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae43c33dece548e0bf7ed28ac92581dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import llm_summarize as llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whipser generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44304b2be8b4bdba8e2da7a7a6fa350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"daryl149/llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you translate the following sentence into English?\n",
      "\n",
      "안녕하세요. 저는 박사입니다.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you translate the following sentence into English?\\n\\n\" + \"안녕하세요. 저는 박사입니다.\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Can you translate the following sentence into English?\n",
      "\n",
      "안녕하세요. 저는 박사입니다.\n",
      "\n",
      "(Note: I'll be using the Revised Romanization of Korean for the translation.)\n",
      "\n",
      "Thanks in advance!\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
